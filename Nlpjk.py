import streamlit as st
import requests
from googletrans import Translator
from pythainlp.tokenize import word_tokenize, sent_tokenize
import re
from transformers import pipeline, AutoTokenizer
import json
import os

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Abstractive Summarization
@st.cache_resource
def load_summarizer():
    try:
        tokenizer = AutoTokenizer.from_pretrained("csebuetnlp/mT5_multilingual_XLSum")
        return pipeline("summarization", model="csebuetnlp/mT5_multilingual_XLSum", tokenizer=tokenizer)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• mT5 ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        try:
            return pipeline("summarization", model="facebook/bart-large-cnn")
        except Exception as e_bart:
            st.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• BART ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e_bart}")
            return None

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏à‡∏≤‡∏Å JSON
@st.cache_resource
def load_glossary():
    glossary_data = {
    "glossary": [
        {
            "en": "Artificial Intelligence (AI)",
            "th": "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÄ‡∏≠‡πÑ‡∏≠)",
            "description": "‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡∏´‡πå ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°"
        },
        {
            "en": "Machine Learning (ML)",
            "th": "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (‡πÄ‡∏≠‡πá‡∏°‡πÅ‡∏≠‡∏•)",
            "description": "‡πÅ‡∏Ç‡∏ô‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á AI ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á"
        },
        {
            "en": "Deep Learning (DL)",
            "th": "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏•‡∏∂‡∏Å (‡πÅ‡∏≠‡∏•)",
            "description": "‡πÅ‡∏Ç‡∏ô‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á Machine Learning ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Ç‡πà‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"
        },
        {
            "en": "Natural Language Processing (NLP)",
            "th": "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤ (‡πÄ‡∏≠‡πá‡∏ô‡πÅ‡∏≠‡∏•)",
            "description": "‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡∏´‡πå ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á"
        },
        {
            "en": "Algorithm",
            "th": "‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°",
            "description": "    "
        },
        {
            "en": "Data",
            "th": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
            "description": "‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"
        },
        {
            "en": "Cloud Computing",
            "th": "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏•‡∏≤‡∏ß‡∏î‡πå",
            "description": "‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï"
        },
        {
            "en": "API (Application Programming Interface)",
            "th": "‡πÄ‡∏≠‡πÑ‡∏≠ (‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå)",
            "description": "‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏Å‡∏é ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ"
        },
        {
            "en": "Blockchain",
            "th": "‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÄ‡∏ä‡∏ô",
            "description": "‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏¢‡πå ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ"
        },
        {
            "en": "Internet of Things (IoT)",
            "th": "‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡∏Ç‡∏≠‡∏á‡∏™‡∏£‡∏£‡∏û‡∏™‡∏¥‡πà‡∏á (‡πÑ‡∏≠‡πÇ‡∏≠‡∏ó‡∏µ)",
            "description": "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô"
        },
        {
            "en": "Cybersecurity",
            "th": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ó‡∏≤‡∏á‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå",
            "description": "‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏à‡∏≤‡∏Å‡πÇ‡∏à‡∏°‡∏ï‡∏µ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏ó‡∏≤‡∏á‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå"
        },
        {
            "en": "Virtual Reality (VR)",
            "th": "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô (‡∏≠‡∏≤‡∏£‡πå)",
            "description": "‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞"
        },
        {
            "en": "Augmented Reality (AR)",
            "th": "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏° (‡πÄ‡∏≠‡∏≠‡∏≤‡∏£‡πå)",
            "description": "‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡πà‡∏ô‡∏†‡∏≤‡∏û‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏ã‡πâ‡∏≠‡∏ô‡∏ö‡∏ô‡πÇ‡∏•‡∏Å ‡πÄ‡∏≠‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
        },
        {
            "en": "Big Data",
            "th": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà",
            "description": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏• ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå"
        },
        {
            "en": "Database",
            "th": "‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
            "description": "‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡πÄ‡∏≠‡∏≤‡πÉ‡∏´‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        },
        {
            "en": "Edge Computing",
            "th": "‡πÄ‡∏≠‡∏î‡∏à‡πå‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå",
            "description": "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Å‡∏≥‡πÄ‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏≠‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ù‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á"
        },
        {
            "en": "Quantum Computing",
            "th": "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏ß‡∏≠‡∏ô‡∏ï‡∏±‡∏°",
            "description": "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≠‡∏ô‡∏ï‡∏±‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå"
        },
        {
            "en": "5G",
            "th": "‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï 5G",
            "description": "‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÑ‡∏£‡πâ‡∏™‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏á‡∏ï‡πà‡∏≥ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å"
        }
    ]
}

    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà
    glossary_file = "tech_glossary.json"
    if not os.path.exists(glossary_file):
        with open(glossary_file, "w", encoding="utf-8") as f:
            json.dump(glossary_data, f, ensure_ascii=False, indent=2)
    else:
        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
        try:
            with open(glossary_file, "r", encoding="utf-8") as f:
                glossary_data = json.load(f)
        except Exception as e:
            st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå glossary: {e}")
    
    return glossary_data

# ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå
def find_in_glossary(term):
    glossary = load_glossary()
    term_lower = term.lower()
    
    for item in glossary["glossary"]:
        if term_lower in item["en"].lower():
            return {
                "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤": item["en"],
                "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•": item["th"],
                "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢": item["description"],
                "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢": item["description"],
                "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢": item["description"],
                "source": "glossary"
            }
    return None

summarizer = load_summarizer()

@st.cache_resource
def load_translator():
    try:
        return Translator()
    except Exception as e:
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î Translator ‡πÑ‡∏î‡πâ: {e}")
        return None

translator = load_translator()

def get_wikipedia_definition(term, lang="en"):
    url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "titles": term,
        "prop": "extracts",
        "exintro": True,
        "explaintext": True,
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        pages = data.get("query", {}).get("pages", {})
        for page in pages.values():
            if "extract" in page and page["extract"].strip():
                return page["extract"]
        return None  # No extract found or empty extract
    except requests.exceptions.RequestException as e:
        st.error(f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Wikipedia: {e}")
        return None
    except json.JSONDecodeError as e:
        st.error(f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏î JSON ‡∏à‡∏≤‡∏Å Wikipedia: {e}")
        return None

def translate_to_thai(text):
    if translator:
        try:
            return translator.translate(text, src='en', dest='th').text
        except:
            return None
    return None

def clean_text(text):
    text = re.sub(r"\(.*?\)", "", text)  # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö
    try:
        tokens = word_tokenize(text, engine="newmm")  # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
        cleaned_tokens = [token.strip() for token in tokens if token.strip()]  # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        return "".join(cleaned_tokens)  # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏ß‡πâ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    except:
        return text

def summarize_text(text, num_sentences=10):
    try:
        sentences = sent_tokenize(text)
        extractive_summary = " ".join(sentences[:num_sentences])
    except:
        return text
    if summarizer:
        try:
            result = summarizer(extractive_summary, max_length=400, min_length=15, do_sample=False)
            return result[0]['summary_text']
        except:
            return None
    return None

def process_word(english_word):
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Å‡πà‡∏≠‡∏ô
    glossary_result = find_in_glossary(english_word)
    if glossary_result:
        return glossary_result

    definition = get_wikipedia_definition(english_word, lang="en")
    if not definition:
        return {"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Wikipedia"}

    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô
    clean_text_EG = clean_text(definition)  # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    summarized_definitions = summarize_text(clean_text_EG)
    if not summarized_definitions:
        return {"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ"}
    
    summarized_definition = translate_to_thai(summarized_definitions)  # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß

    # ‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    thai_translation = translate_to_thai(definition)
    if not thai_translation:
        return {"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ"}

    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    try:
        tokenized_translation = " ".join(word_tokenize(thai_translation, engine="newmm"))
    except:
        tokenized_translation = thai_translation  # ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

    return {
        "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤": english_word,
        "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•": translate_to_thai(english_word),
        "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤DF": definition,
        "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢": tokenized_translation,
        "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢": summarized_definition
    }

# ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Streamlit UI
st.title("üåê TechTerm Translator TH")
st.markdown(f"### üåê ‡πÅ‡∏õ‡∏•‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
st.markdown("‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏• ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢")

word_input = st.text_input("üîç ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå:  ", "")

if st.button("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"):
    if word_input.strip():
        with st.spinner("‚è≥ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."):
            result = process_word(word_input.strip())
        if "error" in result:
            st.error(result["error"])
        else:
            st.success("‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if result.get("source") == "glossary":
                st.info("üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå")
                st.markdown(f"### üî§ ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: `{result['‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤']}`")
                st.markdown(f"**üîÅ ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•:** {result['‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•']}")
                st.markdown("### üìö ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢")
                st.write(result["‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢"])
            else:
                st.info("üåê ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Wikipedia")
                st.markdown(f"### üî§ ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: `{result['‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤']}`")
                st.markdown(f"**üîÅ ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•:** {result['‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•']}")
                st.markdown("### üìö ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢")
                st.write(result["‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤DF"])
                st.markdown("### üìù ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
                st.write(result["‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"])
                st.markdown("### ‚ú® ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢")
                st.write(result["‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢"])
    else:
        st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°")

# ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå
with st.expander("üîß ‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå"):
    st.write("‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÉ‡∏´‡∏°‡πà‡∏•‡∏á‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÑ‡∏î‡πâ")
    
    new_en = st.text_input("‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå", "")
    new_th = st.text_input("‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢", "")
    new_desc = st.text_area("‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢", "")
    
    if st.button("‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå"):
        if new_en and new_th and new_desc:
            glossary = load_glossary()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå3‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß3‡πÑ‡∏°‡πà
            term_exists = False
            for item in glossary["glossary"]:
                if new_en.lower() == item["en"].lower():
                    term_exists = True
                    break
            
            if term_exists:
                st.warning(f"‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå '{new_en}'‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ")
            else:
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÉ‡∏´‡∏°‡πà
                glossary["glossary"].append({
                    "en": new_en,
                    "th": new_th,
                    "description": new_desc
                })
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
                try:
                    with open("tech_glossary.json", "w", encoding="utf-8") as f:
                        json.dump(glossary, f, ensure_ascii=False, indent=2)
                    st.success("‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                except Exception as e:
                    st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: {e}")
        else:
            st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ä‡πà‡∏≠‡∏á")

