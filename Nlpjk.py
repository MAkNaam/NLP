import streamlit as st
import requests
from googletrans import Translator
from pythainlp.tokenize import word_tokenize, sent_tokenize
import re
from transformers import pipeline, AutoTokenizer
import json

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Abstractive Summarization
@st.cache_resource
def load_summarizer():
    try:
        tokenizer = AutoTokenizer.from_pretrained("csebuetnlp/mT5_multilingual_XLSum")
        return pipeline("summarization", model="csebuetnlp/mT5_multilingual_XLSum", tokenizer=tokenizer)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• mT5 ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        try:
            return pipeline("summarization", model="facebook/bart-large-cnn")
        except Exception as e_bart:
            st.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• BART ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e_bart}")
            return None

summarizer = load_summarizer()

@st.cache_resource
def load_translator():
    try:
        return Translator()
    except Exception as e:
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î Translator ‡πÑ‡∏î‡πâ: {e}")
        return None

translator = load_translator()

def get_wikipedia_definition(term, lang="en"):
    url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "titles": term,
        "prop": "extracts",
        "exintro": True,
        "explaintext": True,
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        pages = data.get("query", {}).get("pages", {})
        for page in pages.values():
            if "extract" in page:
                return page["extract"]
    except:
        return None

def translate_to_thai(text):
    if translator:
        try:
            return translator.translate(text, src='en', dest='th').text
        except:
            return None
    return None

def clean_text(text):
    text = re.sub(r"\(.*?\)", "", text)
    try:
        tokens = word_tokenize(text, engine="newmm")
        return " ".join(tokens)
    except:
        return text

def summarize_text(text, num_sentences=2):
    try:
        sentences = sent_tokenize(text)
        extractive_summary = " ".join(sentences[:num_sentences])
    except:
        return text
    if summarizer:
        try:
            result = summarizer(extractive_summary, max_length=50, min_length=15, do_sample=False)
            return result[0]['summary_text']
        except:
            return None
    return None

def process_word(english_word):
    definition = get_wikipedia_definition(english_word, lang="en")
    if not definition:
        return {"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Wikipedia"}

    thai_translation = translate_to_thai(definition)
    if not thai_translation:
        return {"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ"}

    cleaned_definition = clean_text(thai_translation)
    summarized_definition = summarize_text(cleaned_definition)

    return {
        "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤": english_word,
        "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•": translate_to_thai(english_word),
        "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©": definition,
        "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡∏∏‡∏õ": cleaned_definition,
        "‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ": summarized_definition if summarized_definition else "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ"
    }

# ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Streamlit UI
st.title("üåê ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏à‡∏≤‡∏Å Wikipedia ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
st.markdown("‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ ‡πÅ‡∏õ‡∏• ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ")

word_input = st.text_input("üîç ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©", "")

if st.button("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ"):
    if word_input.strip():
        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."):
            result = process_word(word_input.strip())
        if "error" in result:
            st.error(result["error"])
        else:
            st.success("‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            st.markdown(f"### üî§ ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: `{result['‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤']}`")
            st.markdown(f"**üîÅ ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•:** {result['‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•']}")
            st.markdown("### üìö ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©")
            st.write(result["‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©"])
            st.markdown("### üìù ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡∏∏‡∏õ)")
            st.write(result["‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡∏∏‡∏õ"])
            st.markdown("### ‚ú® ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ)")
            st.write(result["‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ"])
    else:
        st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°")
